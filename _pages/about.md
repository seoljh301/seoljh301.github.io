---
title: "About"
permalink: /about/
author_profile: true
---

I am a researcher focused on bridging the gap between **acoustic signal processing** and **semantic reasoning** to build fair, robust, and interpretable audio intelligence. 

With a background in sound design and programming, I am fascinated by how acoustic features—such as pitch, prosody, and speaker identity—interact with language models. My current research explores the internal mechanisms of speech-aware LLMs, specifically targeting **acoustic social bias** and **mechanistic interpretability**.

## Research Interests
* **Trustworthy & Fair SLMs:** Investigating and mitigating acoustic-based social bias in spoken language models.
* **Mechanistic Interpretability:** Layer-wise probing of audio foundation models (e.g., Whisper) to understand information encoding.
* **Robust Target Speech Extraction:** Developing text-prompted TSE architectures that remain robust in complex cocktail party scenarios.
* **Automated Evaluation:** Building multi-dimensional, human-centric evaluation frameworks (AutoMOS) using LLMs.

## Education
* **B.S. in Computer Science and Engineering**, Chung-Ang University (2019-2025)

## Featured Publications

### Trustworthy AI
* **VoiceBBQ** **(EMNLP 2025 Main)**
  * A comprehensive benchmark for diagnosing content and acoustic social bias in Spoken LMs.
* **Acoustic-based Gender Differentiation** (Under revision)
  * Systematic analysis of how identical questions lead to different responses based on speaker gender.
* **FairSLM** (In progress)
  * Proposed a Social Bias Disentanglement Module (SBDM) to mitigate gender and identity bias in SLM architectures.

### Robust Speech Processing
* **VORTEX & PORTE Dataset** (Under revision)
  * Robust text-prompted TSE model using Dynamic Allocation Multi-branch (DAM) Fusion for various overlap conditions.

### Interpretability & Evaluation
* **Probing Whisper** (In progress)
  * Layer-wise analysis of how speech encoders process and preserve acoustic cues like emotion and prosody.
* **AutoMOS** (In progress)
  * Zero-shot, multi-dimensional speech quality assessment framework leveraging LLM reasoning.

## Projects
* **StyleTTS2 + PromptTTS Implementation**
  * Developed a SOTA prompt-conditioned TTS system by integrating StyleTTS2 with PromptTTS architectures.
* **E3TTS-based Paraverbal Speech Generation**
  * Researching diffusion-based paraverbal speech generation using E3-TTS framework.
* **AudioPoli: Auditory Emergency-Scene Classification** **(Solution Challenge Global Top 100)**
  * Developed an environmental sound classification system for emergency situation detection.
* **AULO: Short-length Sound Effect Retrieval**
  * Implemented a sound effect retrieval system using VAE-based encoders.

## Awards & Honors
* **Solution Challenge - Global Top 100**, Google Developers Groups (2024)
* **Full-tuition Merit-based Scholarship**, Chung-Ang University (2019 - 2025)

## Skills
* **Programming:** Python, C/C++, Bash, CUDA basics, Dart (Flutter), SQL, etc.
* **Frameworks:** PyTorch, TensorFlow, Keras, etc.
* **Toolkits:** HuggingFace Transformers, Librosa, OpenAI Whisper, ESPnet, Kaldi, Amphion, etc.
* **Infrastructure:** PyTorch Lightning, Docker, GCP, Git, etc.

## Contact
* **Email:** `seoljh301@gmail.com` / `seoljh301@cau.ac.kr`
* **GitHub:** [github.com/seoljh301](https://github.com/seoljh301)